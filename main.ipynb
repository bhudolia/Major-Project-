{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b810c24b4ec3721f1ba21f8533ccdb9fa77a544"
   },
   "source": [
    "# Sentiment Analysis on Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdb35c375feb9da290ef70440ed819ed6154be1e"
   },
   "source": [
    "In this notebook Sentiment Analysis is performed on movie reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e383496b9b5848f85d5a8508e20727e0f596b3c7",
    "toc": true
   },
   "source": [
    "<h1>Content<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Import</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Selection</a></span></li><li><span><a href=\"#Model-Architecture\" data-toc-modified-id=\"Model-Architecture-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Architecture</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-&amp;-Loss\" data-toc-modified-id=\"Accuracy-&amp;-Loss-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Accuracy &amp; Loss</a></span></li><li><span><a href=\"#Error-Analysis\" data-toc-modified-id=\"Error-Analysis-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Error Analysis</a></span></li></ul></li><li><span><a href=\"#Model-Application\" data-toc-modified-id=\"Model-Application-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Model Application</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-Predictions\" data-toc-modified-id=\"Test-Predictions-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Test Predictions</a></span></li><li><span><a href=\"#Custom-Reviews\" data-toc-modified-id=\"Custom-Reviews-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Custom Reviews</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "ef3ea28d8941735e353645478b9844df25497840"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-838260f70abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#from tensorflow.python.keras.layers import Dense, Dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#from tensorflow.python.keras import optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "#from tensorflow.python.keras.layers import Dense, Dropout\n",
    "#from tensorflow.python.keras import optimizers\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet \n",
    "allEnglishWords = words.words() + [w for w in wordnet.words()]\n",
    "allEnglishWords = np.unique([x.lower() for x in allEnglishWords])\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24e1f135256fa1e76453e3fbb302497f0486771f"
   },
   "source": [
    "---\n",
    "\n",
    "## Data Import\n",
    "First, we need to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "1481bb91fbe6ab54874254b96127a9ea0a28f590"
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "positiveFiles = [x for x in os.listdir(path+\"train/pos/\") if x.endswith(\".txt\")]\n",
    "negativeFiles = [x for x in os.listdir(path+\"train/neg/\") if x.endswith(\".txt\")]\n",
    "testFiles = [x for x in os.listdir(path+\"test/\") if x.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "b05c12defc2790556abcd81b183750ca6badb14a"
   },
   "outputs": [],
   "source": [
    "positiveReviews, negativeReviews, testReviews = [], [], []\n",
    "for pfile in positiveFiles:\n",
    "    with open(path+\"train/pos/\"+pfile, encoding=\"latin1\") as f:\n",
    "        positiveReviews.append(f.read())\n",
    "for nfile in negativeFiles:\n",
    "    with open(path+\"train/neg/\"+nfile, encoding=\"latin1\") as f:\n",
    "        negativeReviews.append(f.read())\n",
    "for tfile in testFiles:\n",
    "    with open(path+\"test/\"+tfile, encoding=\"latin1\") as f:\n",
    "        testReviews.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "d1b2a5e2df68adaa443eb82365b340c4bac8879b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21939</th>\n",
       "      <td>\"National Lampoon Goes to the Movies\" is the w...</td>\n",
       "      <td>0</td>\n",
       "      <td>7277_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24113</th>\n",
       "      <td>I can't believe that so much talent can be was...</td>\n",
       "      <td>0</td>\n",
       "      <td>1073_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>This is a wonderful film. The non-stop patter ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10574_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17240</th>\n",
       "      <td>Did anyone who was making this movie, particul...</td>\n",
       "      <td>0</td>\n",
       "      <td>1085_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>While a bit preachy on the topic of progress a...</td>\n",
       "      <td>1</td>\n",
       "      <td>8127_8.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label          file\n",
       "21939  \"National Lampoon Goes to the Movies\" is the w...      0    7277_1.txt\n",
       "24113  I can't believe that so much talent can be was...      0    1073_2.txt\n",
       "4633   This is a wonderful film. The non-stop patter ...      1  10574_10.txt\n",
       "17240  Did anyone who was making this movie, particul...      0    1085_2.txt\n",
       "4894   While a bit preachy on the topic of progress a...      1    8127_8.txt"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.concat([\n",
    "    pd.DataFrame({\"review\":positiveReviews, \"label\":1, \"file\":positiveFiles}),\n",
    "    pd.DataFrame({\"review\":negativeReviews, \"label\":0, \"file\":negativeFiles}),\n",
    "    pd.DataFrame({\"review\":testReviews, \"label\":-1, \"file\":testFiles})\n",
    "], ignore_index=True).sample(frac=1, random_state=1)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4f08be82b084c106ddb089f054bf257022c1ba2"
   },
   "source": [
    "With everything centralized in 1 dataframe, we now perform train, validation and test set splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "b719efc8831a811073ea75277f72b62789d1e0e0"
   },
   "outputs": [],
   "source": [
    "reviews = reviews[[\"review\", \"label\", \"file\"]].sample(frac=1, random_state=1)\n",
    "train = reviews[reviews.label!=-1].sample(frac=0.6, random_state=1)\n",
    "valid = reviews[reviews.label!=-1].drop(train.index)\n",
    "test = reviews[reviews.label==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "27d24bf72bd27f9d1fb5625aa6fd65da34c0e1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 3)\n",
      "(10000, 3)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "b0a081b837717a864ae9ad1c30cf5aa01acb8c4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "This documentary explores a story covered in Pilger's latest book \"Freedom Next Time\", which was published in 2006. It reveals the shocking expulsion of the natives of Diego Garcia, one of the Chagos Islands in the Indian Ocean.<br /><br />The islanders are technically British citizens, as Diego Garcia is a British colony, much like Mauritius, the nearby island to where the natives were exiled, used to be. But the British government has ignored their pleas to return to their homeland, as the island is now a military base for the United States army, who have used it as a basis for the bombing of Iraq and Afghanistan.<br /><br />As usual, Pilger's coverage is shocking, especially as he documents the treatment and the current impoverished living conditions of the surviving islanders. His interviews all round are excellent, and his cornering of a Parliament representative where he uses the Government's own information to pin him down, ranks as one of his best.<br /><br />Pilger also uses dramatic reconstruction to dissect a series of recently released documents that fully illuminate the British conspiracy to evict the natives. The weaving of this footage with the interviews, and the islanders music, really heightens the film's impact.<br /><br />It is not easy viewing, but \"Stealing a Nation\" is John Pilger at his best. Recommended."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(train.review.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc2aa83cb2974a57d696e3481eb3ae62082be434"
   },
   "source": [
    "---\n",
    "\n",
    "## Data Preprocessing\n",
    "The next step is data preprocessing. The following class behaves like your typical SKLearn vectorizer.\n",
    "\n",
    "It can perform the following operations.\n",
    "* Discard non alpha-numeric characters\n",
    "* Set everything to lower case\n",
    "* Stems all words using PorterStemmer, and change the stems back to the most occurring existent word.\n",
    "* Discard non-Egnlish words (not by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "5fcdb0c46e96a0abdf3ef83e8c27c02625a58b0d"
   },
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    ''' Preprocess data for NLP tasks. '''\n",
    "\n",
    "    def __init__(self, alpha=True, lower=True, stemmer=True, english=False):\n",
    "        self.alpha = alpha\n",
    "        self.lower = lower\n",
    "        self.stemmer = stemmer\n",
    "        self.english = english\n",
    "        \n",
    "        self.uniqueWords = None\n",
    "        self.uniqueStems = None\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "\n",
    "        allwords = pd.DataFrame({\"word\": np.concatenate(texts.apply(lambda x: x.split()).values)})\n",
    "        self.uniqueWords = allwords.groupby([\"word\"]).size().rename(\"count\").reset_index()\n",
    "        self.uniqueWords = self.uniqueWords[self.uniqueWords[\"count\"]>1]\n",
    "        if self.stemmer:\n",
    "            self.uniqueWords[\"stem\"] = self.uniqueWords.word.apply(lambda x: PorterStemmer().stem(x)).values\n",
    "            self.uniqueWords.sort_values([\"stem\", \"count\"], inplace=True, ascending=False)\n",
    "            self.uniqueStems = self.uniqueWords.groupby(\"stem\").first()\n",
    "        \n",
    "        #if self.english: self.words[\"english\"] = np.in1d(self.words[\"mode\"], allEnglishWords)\n",
    "        print(\"Fitted.\")\n",
    "            \n",
    "    def transform(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "        if self.stemmer:\n",
    "            allwords = np.concatenate(texts.apply(lambda x: x.split()).values)\n",
    "            uniqueWords = pd.DataFrame(index=np.unique(allwords))\n",
    "            uniqueWords[\"stem\"] = pd.Series(uniqueWords.index).apply(lambda x: PorterStemmer().stem(x)).values\n",
    "            uniqueWords[\"mode\"] = uniqueWords.stem.apply(lambda x: self.uniqueStems.loc[x, \"word\"] if x in self.uniqueStems.index else \"\")\n",
    "            texts = texts.apply(lambda x: \" \".join([uniqueWords.loc[y, \"mode\"] for y in x.split()]))\n",
    "        #if self.english: texts = self.words.apply(lambda x: \" \".join([y for y in x.split() if self.words.loc[y,\"english\"]]))\n",
    "        print(\"Transformed.\")\n",
    "        return(texts)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "        self.fit(texts)\n",
    "        texts = self.transform(texts)\n",
    "        return(texts)\n",
    "    \n",
    "    def _doAlways(self, texts):\n",
    "        # Remove parts between <>'s\n",
    "        texts = texts.apply(lambda x: re.sub('<.*?>', ' ', x))\n",
    "        # Keep letters and digits only.\n",
    "        if self.alpha: texts = texts.apply(lambda x: re.sub('[^a-zA-Z0-9 ]+', ' ', x))\n",
    "        # Set everything to lower case\n",
    "        if self.lower: texts = texts.apply(lambda x: x.lower())\n",
    "        return texts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "032e6ed630e9d0b94ceca801fb264d857707b1dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6011</th>\n",
       "      <td>This documentary explores a story covered in P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1251_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9653</th>\n",
       "      <td>Released two years before I was born, this Osc...</td>\n",
       "      <td>1</td>\n",
       "      <td>11396_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15040</th>\n",
       "      <td>THE ZOMBIE CHRONICLES &lt;br /&gt;&lt;br /&gt;Aspect ratio...</td>\n",
       "      <td>0</td>\n",
       "      <td>94_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6029</th>\n",
       "      <td>it's amazing that so many people that i know h...</td>\n",
       "      <td>1</td>\n",
       "      <td>8654_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>This final entry in George Lucas's STAR WARS m...</td>\n",
       "      <td>1</td>\n",
       "      <td>820_10.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label          file\n",
       "6011   This documentary explores a story covered in P...      1    1251_9.txt\n",
       "9653   Released two years before I was born, this Osc...      1  11396_10.txt\n",
       "15040  THE ZOMBIE CHRONICLES <br /><br />Aspect ratio...      0      94_1.txt\n",
       "6029   it's amazing that so many people that i know h...      1    8654_9.txt\n",
       "9729   This final entry in George Lucas's STAR WARS m...      1    820_10.txt"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "df22fbf78cbc9330c7d4d842544a41111822aefa"
   },
   "outputs": [],
   "source": [
    "preprocess = Preprocessor(alpha=True, lower=True, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "079f43c870081d6596bb1acae19c7346e9a31574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted.\n",
      "Transformed.\n",
      "Transformed.\n",
      "CPU times: user 1min 16s, sys: 2.79 s, total: 1min 19s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainX = preprocess.fit_transform(train.review)\n",
    "validX = preprocess.transform(valid.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "af61c30e6ceb97b44ffd75ad2ea92024010c945f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6011     this documentary explore a story cover in pilg...\n",
       "9653     released two years before i was born this osca...\n",
       "15040    the zombie chronicles aspect ratio 1 33 1 nu v...\n",
       "6029     it s amazing that so many people that i know h...\n",
       "9729     this finally entry in george lucas s star war ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "c9768ca9cae490e60bd3d35398645eb6c8fd228f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38298, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15173</th>\n",
       "      <td>disappointingly</td>\n",
       "      <td>9</td>\n",
       "      <td>disappointingli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15171</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>527</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15174</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>271</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15172</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>235</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15170</th>\n",
       "      <td>disappoint</td>\n",
       "      <td>62</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15177</th>\n",
       "      <td>disappoints</td>\n",
       "      <td>19</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15176</th>\n",
       "      <td>disappointments</td>\n",
       "      <td>14</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  count             stem\n",
       "15173  disappointingly      9  disappointingli\n",
       "15171     disappointed    527       disappoint\n",
       "15174   disappointment    271       disappoint\n",
       "15172    disappointing    235       disappoint\n",
       "15170       disappoint     62       disappoint\n",
       "15177      disappoints     19       disappoint\n",
       "15176  disappointments     14       disappoint"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preprocess.uniqueWords.shape)\n",
    "preprocess.uniqueWords[preprocess.uniqueWords.word.str.contains(\"disappoint\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "38a34b398e3199006c90d9f8318c14f6cc1cc370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25381, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stem</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>disappoint</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointingli</th>\n",
       "      <td>disappointingly</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word  count\n",
       "stem                                   \n",
       "disappoint          disappointed    527\n",
       "disappointingli  disappointingly      9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preprocess.uniqueStems.shape)\n",
    "preprocess.uniqueStems[preprocess.uniqueStems.word.str.contains(\"disappoint\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44b462bbff3ae1fc92cac57b9f4ded43019018c8"
   },
   "source": [
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "Next, we take the preprocessed texts as input and calculate their TF-IDF's ([info](http://www.tfidf.com)). We retain 10000 features per text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "87ea6e08cfabc83b677977c6cbf2a9ad1facacd6"
   },
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union([\"thats\",\"weve\",\"dont\",\"lets\",\"youre\",\"im\",\"thi\",\"ha\",\n",
    "    \"wa\",\"st\",\"ask\",\"want\",\"like\",\"thank\",\"know\",\"susan\",\"ryan\",\"say\",\"got\",\"ought\",\"ive\",\"theyre\"])\n",
    "tfidf = TfidfVectorizer(min_df=2, max_features=10000, stop_words=stop_words) #, ngram_range=(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "26bb2efc663fbfa4980b97421b6fcf5d8aa95aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.59 s, sys: 892 ms, total: 5.49 s\n",
      "Wall time: 6.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainX = tfidf.fit_transform(trainX).toarray()\n",
    "validX = tfidf.transform(validX).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "2bdd00347485d4e5e3db5cd775162409899e534f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 10000)\n",
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(validX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "7684ada76ae9ab81c17987ac480680a2393e7585"
   },
   "outputs": [],
   "source": [
    "trainY = train.label\n",
    "validY = valid.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "8a03774904396e71738cee11658a0f6fea4eda52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 10000) (15000,)\n",
      "(10000, 10000) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, trainY.shape)\n",
    "print(validX.shape, validY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66bb65fc219ab8c5e8bc0b9eca79ea2fa77911f4"
   },
   "source": [
    "---\n",
    "\n",
    "## Feature Selection\n",
    "Next, we take the 10k dimensional tfidf's as input, and keep the 2000 dimensions that correlate the most with our sentiment target. The corresponding words - see below - make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "d921ce51e8012db3d74511741c3c61fe1ca9f08a"
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "efe0c4d4d870f4d6d5d9b367111e45d6503a389d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01394664 -0.02379348  0.01151353 ...  0.01492592  0.02289527\n",
      "  0.00199103]\n"
     ]
    }
   ],
   "source": [
    "getCorrelation = np.vectorize(lambda x: pearsonr(trainX[:,x], trainY)[0])\n",
    "correlations = getCorrelation(np.arange(trainX.shape[1]))\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "bc89e481eb151a16964b4cd725fad86a3446dfb4"
   },
   "outputs": [],
   "source": [
    "allIndeces = np.argsort(-correlations)\n",
    "bestIndeces = allIndeces[np.concatenate([np.arange(1000), np.arange(-1000, 0)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "9bedebdc1406e24d21ef775af93ce16d01d1ff5e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great' 'love' 'excellent' 'beautiful' 'best' 'perfect' 'favorite'\n",
      " 'enjoy' 'amazing' 'performance']\n",
      "['minutes' 'poor' 'horrible' 'worse' 'terrible' 'boring' 'awful' 'waste'\n",
      " 'worst' 'bad']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = np.array(tfidf.get_feature_names())\n",
    "print(vocabulary[bestIndeces][:10])\n",
    "print(vocabulary[bestIndeces][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "5ec3ab480f29519cf862400e472d494ef59c6afe"
   },
   "outputs": [],
   "source": [
    "trainX = trainX[:,bestIndeces]\n",
    "validX = validX[:,bestIndeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "ea076abbc8e375b7f5887e545c2a93e99a74039c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 2000) (15000,)\n",
      "(10000, 2000) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, trainY.shape)\n",
    "print(validX.shape, validY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "851531479bd0741b7b85f0d66feb518f5e8f6884"
   },
   "source": [
    "---\n",
    "\n",
    "## Model Architecture\n",
    "We choose a very simple dense network with 6 layers, performing binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "4484d9f286282862d7f63fe743590d427ef8f87e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-6710c710970a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mACTIVATION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = Sequential([    \n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACTIVATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "DROPOUT = 0.5\n",
    "ACTIVATION = \"tanh\"\n",
    "\n",
    "model = Sequential([    \n",
    "    Dense(int(trainX.shape[1]/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(int(trainX.shape[1]/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(int(trainX.shape[1]/4), activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(100, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(20, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(5, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "683cda4b76b1f12887e944f877b69d52d428edbe"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(0.00005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a9bbc915144f49016162c7b48d1f6d0fab1fc0d"
   },
   "source": [
    "---\n",
    "\n",
    "## Model Training\n",
    "Let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2dd8f0aa6829bca51b23c6d90192617d77bd4cd9"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCHSIZE = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6276fd33bf3571a528bcc5d8cc816264b70e021e"
   },
   "outputs": [],
   "source": [
    "model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCHSIZE, validation_data=(validX, validY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65c8bb930bdd202430bb3a005f9d2d28b7bbd300"
   },
   "outputs": [],
   "source": [
    "x = np.arange(EPOCHS)\n",
    "history = model.history.history\n",
    "\n",
    "data = [\n",
    "    go.Scatter(x=x, y=history[\"acc\"], name=\"Train Accuracy\", marker=dict(size=5), yaxis='y2'),\n",
    "    go.Scatter(x=x, y=history[\"val_acc\"], name=\"Valid Accuracy\", marker=dict(size=5), yaxis='y2'),\n",
    "    go.Scatter(x=x, y=history[\"loss\"], name=\"Train Loss\", marker=dict(size=5)),\n",
    "    go.Scatter(x=x, y=history[\"val_loss\"], name=\"Valid Loss\", marker=dict(size=5))\n",
    "]\n",
    "layout = go.Layout(\n",
    "    title=\"Model Training Evolution\", font=dict(family='Palatino'), xaxis=dict(title='Epoch', dtick=1),\n",
    "    yaxis1=dict(title=\"Loss\", domain=[0, 0.45]), yaxis2=dict(title=\"Accuracy\", domain=[0.55, 1]),\n",
    ")\n",
    "py.iplot(go.Figure(data=data, layout=layout), show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c228a8d8681ba328cfa5359b5f943be713def86"
   },
   "source": [
    "---\n",
    "\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a39927e07ab1483b90464c5413bd9f862ba5e4c"
   },
   "source": [
    "### Accuracy & Loss\n",
    "Let's first centralize the probabilities and predictions with the original train and validation dataframes. Then we can print out the respective accuracies and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01f2979207282c2e4c718f60894d97b6baeeccc9"
   },
   "outputs": [],
   "source": [
    "train[\"probability\"] = model.predict(trainX)\n",
    "train[\"prediction\"] = train.probability-0.5>0\n",
    "train[\"truth\"] = train.label==1\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f185efce8414c7f11badedc9c6a0ef62584a6785"
   },
   "outputs": [],
   "source": [
    "print(model.evaluate(trainX, trainY))\n",
    "print((train.truth==train.prediction).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f5fd5b9449e4be3285280e2691b58f77a81014e"
   },
   "outputs": [],
   "source": [
    "valid[\"probability\"] = model.predict(validX)\n",
    "valid[\"prediction\"] = valid.probability-0.5>0\n",
    "valid[\"truth\"] = valid.label==1\n",
    "valid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12d1a5c10492f9bb721a0cea3d553b1813186376"
   },
   "outputs": [],
   "source": [
    "print(model.evaluate(validX, validY))\n",
    "print((valid.truth==valid.prediction).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65734bdfebc04778bacfdcbe866ae881ffed33de"
   },
   "source": [
    "### Error Analysis\n",
    "Error analysis gives us great insight in the way the model is making its errors. Often, it shows data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9161fae002303d3947f31b10f0c8421641fb4f2"
   },
   "outputs": [],
   "source": [
    "trainCross = train.groupby([\"prediction\", \"truth\"]).size().unstack()\n",
    "trainCross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d4454b148ac449b92296a6672cd331566f85233"
   },
   "outputs": [],
   "source": [
    "validCross = valid.groupby([\"prediction\", \"truth\"]).size().unstack()\n",
    "validCross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a903053503fff95fb55527f82c8dcb1b55323652"
   },
   "outputs": [],
   "source": [
    "truepositives = valid[(valid.truth==True)&(valid.truth==valid.prediction)]\n",
    "print(len(truepositives), \"true positives.\")\n",
    "truepositives.sort_values(\"probability\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "493772512c1c7fbf62b11960ae8bc5c4e29a2c6c"
   },
   "outputs": [],
   "source": [
    "truenegatives = valid[(valid.truth==False)&(valid.truth==valid.prediction)]\n",
    "print(len(truenegatives), \"true negatives.\")\n",
    "truenegatives.sort_values(\"probability\", ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d58e3a6900933a499276c6431b18cf56fc57ad77"
   },
   "outputs": [],
   "source": [
    "falsepositives = valid[(valid.truth==True)&(valid.truth!=valid.prediction)]\n",
    "print(len(falsepositives), \"false positives.\")\n",
    "falsepositives.sort_values(\"probability\", ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6146fa59be349d7443db78398f4ef222bf246feb"
   },
   "outputs": [],
   "source": [
    "falsenegatives = valid[(valid.truth==False)&(valid.truth!=valid.prediction)]\n",
    "print(len(falsenegatives), \"false negatives.\")\n",
    "falsenegatives.sort_values(\"probability\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "090480548ce39851c39c514f28482fd9c2e4a0f8"
   },
   "source": [
    "This is the review that got predicted as positive most certainly - while being labeled as negative. However, we can easily recognize it as a poorly labeled sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "521b966abe3ad3031a1824bc7b713d7f65eb708f"
   },
   "outputs": [],
   "source": [
    "HTML(valid.loc[22148].review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa2a5414267cb6b69ef3f73c44611641df51d71f"
   },
   "source": [
    "---\n",
    "\n",
    "## Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e908ad396fdc1bb655c2cff462fffdb31f73245e"
   },
   "source": [
    "### Custom Reviews\n",
    "To use this model, we would store the model, along with the preprocessing vectorizers, and run the unseen texts through following pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "707e470e97da1bb0a9b7b6692c5bfbc2cec31f44"
   },
   "outputs": [],
   "source": [
    "unseen = pd.Series(\"this movie very good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84fc80a824af3ea9971c05fbb398386df8f2d3f0"
   },
   "outputs": [],
   "source": [
    "unseen = preprocess.transform(unseen)       # Text preprocessing\n",
    "unseen = tfidf.transform(unseen).toarray()  # Feature engineering\n",
    "unseen = unseen[:,bestIndeces]              # Feature selection\n",
    "probability = model.predict(unseen)[0,0]  # Network feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b02ab42a16ad85ba33a2fc4793e09adaed7d2ad"
   },
   "outputs": [],
   "source": [
    "print(probability)\n",
    "print(\"Positive!\") if probability > 0.5 else print(\"Negative!\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Content",
   "title_sidebar": "Content",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
